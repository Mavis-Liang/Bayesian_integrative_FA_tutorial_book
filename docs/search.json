[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A Tutorial for Bayesian Integrative Factor Models",
    "section": "",
    "text": "Welcome\nThis is the tutorial to guide the use of Bayesian integrative factor models. Including naive methods Stack FA (Stacking data from all the studies and apply FA), Ind FA (applying FA to each study separatly), and advanced methods PFA (Perturbed Factor Analysis) (Roy et al. 2021), MOM-SS (Bayesian Factor Regression with the non-local spike-and-slab priors) (Avalos-Pacheco, Rossell, and Savage 2022), SUFA (Subspace Factor Analysis) (Chandra, Dunson, and Xu 2024), BMSFA (Bayesian Multi-study Factor Analysis) (De Vito et al. 2021) and Tetris (Grabski et al. 2023).\nFull intro to the multi-study data and comparison for these methods can be found in the manuscript. Raw code can be found the the GitHub repository (https://github.com/Mavis-Liang/Bayesian_integrative_FA_tutorial/tree/main)",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#quick-start",
    "href": "index.html#quick-start",
    "title": "A Tutorial for Bayesian Integrative Factor Models",
    "section": "Quick start",
    "text": "Quick start\nStep 1: Prepare the package and data\n\n# install.packages(\"remotes\")\n# remotes::install_github(\"rdevito/MSFA\")\nlibrary(MSFA)\n\nA small simulated data can be downloaded here:\nA small simulated data (if it does not trigger instant download, you can go to the repo provided above and find it under folder RDS).\n\ndata &lt;- readRDS(\"Data/sim_B_small.rds\")\n\nA glance at the data:\n\ndim(data$Y_list[[1]])\n\n[1] 50 20\n\n\nFor study 1, we have 50 samples and 20 features.\n\nlength(data$Y_list)\n\n[1] 3\n\n\nWe have 3 studies in total.\n\nsapply(data$Y_list, dim)\n\n     [,1] [,2] [,3]\n[1,]   50   50   50\n[2,]   20   20   20\n\n\nThe second and third studies also have 50 samples and 20 features.\nStep 2: Run BMSFA\n\nfit1 &lt;- MSFA::sp_msfa(data$Y_list, k = 5, j_s = c(4, 4, 4),\n                      scaling = FALSE, centering = TRUE,\n                      control = list(nrun = 5000, burn = 4000))\n\nr= 1000 \nr= 2000 \nr= 3000 \nr= 4000 \nr= 5000 \n\n\nStep 3: Post-processing\nWe use sp_OP to apply orthogonal Procrustes rotation to the posterior samples of the common loading matrix \\(\\Phi\\) and the study-specific loading matrices \\(\\Lambda_s\\). The function sp_OP is from the MSFA package.\nCommon covariance and study-specific covariance matrices are calculated by the cross-product of the loading matrices.\nThe marginal covariance matrix is calculated by \\(\\Sigma_s = \\Phi \\Phi^T +  \\Lambda_s \\Lambda_s^T + diag(\\Psi_s)\\), where \\(\\Psi_s\\) is the diagonal matrix of the residual variances for study \\(s\\).\n\nlibrary(tidyverse) # for data wrangling\npost_BMSFA &lt;- function(fit){\n  # Common covariance matrix and loading\n  est_Phi &lt;- MSFA::sp_OP(fit$Phi, trace=FALSE)$Phi\n  est_SigmaPhi &lt;- tcrossprod(est_Phi)\n  \n  # Study-specific covariance matrices and loadings\n  est_LambdaList &lt;- lapply(fit$Lambda, function(x) MSFA::sp_OP(x, trace=FALSE)$Phi)\n  est_SigmaLambdaList &lt;- lapply(est_LambdaList, function(x) tcrossprod(x))\n  \n  # Marginal covariance matrices\n  S &lt;- length(est_SigmaLambdaList)\n  # Get point estimate of each Psi_s\n  est_PsiList &lt;- lapply(1:S, function(s) {\n    apply(fit$psi[[s]], c(1, 2), mean)\n  })\n  est_margin_cov &lt;- lapply(1:S, function(s) {\n    est_SigmaPhi + est_SigmaLambdaList[[s]] + diag(est_PsiList[[s]] %&gt;% as.vector())\n  })\n  \n  return(list(Phi = est_Phi, SigmaPhi = est_SigmaPhi,\n         LambdaList = est_LambdaList, SigmaLambdaList = est_SigmaLambdaList,\n         PsiList = est_PsiList,\n         SigmaMarginal = est_margin_cov))\n}\nout1 &lt;- post_BMSFA(fit1)\n\nNow, determine the number of factors using eigen value decomposition (EVD).\n\n# Obtain the eigen values from the common covariance matrix\nval_eigen_SigmaPhi &lt;- eigen(out1$SigmaPhi)$values\n# Proportion of variance explained by each eigen value\nprop_var_SigmaPhi &lt;- val_eigen_SigmaPhi/sum(val_eigen_SigmaPhi)\n# Choose the number of factors - factors that explain more than 5% of the variance\nchoose_K_SigmaPhi &lt;- length(which(prop_var_SigmaPhi &gt; 0.05))\n\n# Similarly, for each study-specific covariance matrix:\nval_eigen_SigmaLambdaList &lt;- lapply(out1$SigmaLambdaList, function(x) eigen(x)$values)\nprop_var_SigmaLambdaList &lt;- lapply(val_eigen_SigmaLambdaList, function(x) x/sum(x))\nchoose_K_SigmaLambdaList &lt;- lapply(prop_var_SigmaLambdaList, function(x) length(which(x &gt; 0.05)))\n\n# Print the output\nchoose_K_SigmaPhi\n\n[1] 4\n\nfor(s in 1:length(choose_K_SigmaLambdaList)){\n  cat(\"Study\", s, \"has\", choose_K_SigmaLambdaList[[s]], \"factors.\\n\")\n}\n\nStudy 1 has 1 factors.\nStudy 2 has 1 factors.\nStudy 3 has 2 factors.\n\n\nNow we rerun the model with the number of factors we just determined. And again we post-process the MCMC chains with post_BMSFA().\n\nfit2 &lt;- MSFA::sp_msfa(data$Y_list, k = 4, j_s = c(1, 1, 2),\n                      scaling = FALSE, centering = TRUE,\n                      control = list(nrun = 5000, burn = 4000))\n\nr= 1000 \nr= 2000 \nr= 3000 \nr= 4000 \nr= 5000 \n\nout2 &lt;- post_BMSFA(fit2)\n\nStep 4: Check our results\n\n# For example, view the loading matrix\nhead(out2$Phi)\n\n            [,1]         [,2]       [,3]      [,4]\n[1,]  0.16952713  0.020636365 -1.1875126 0.3430111\n[2,] -0.08185122  0.117941438 -1.2051410 0.3426028\n[3,] -0.24166569  0.009744298 -0.9142091 0.3266155\n[4,] -0.23449935  0.057977792 -0.7576149 0.1696956\n[5,] -0.30719242  0.025246159 -0.4643306 0.2115390\n[6,] -0.42761733 -0.030917582 -0.3497078 0.2136760\n\ndim(out2$Phi)\n\n[1] 20  4\n\n\n\n\n\n\nAvalos-Pacheco, Alejandra, David Rossell, and Richard S. Savage. 2022. “Heterogeneous Large Datasets Integration Using Bayesian Factor Regression.” Bayesian Analysis 17 (1): 33–66.\n\n\nChandra, Noirrit Kiran, David B Dunson, and Jason Xu. 2024. “Inferring Covariance Structure from Multiple Data Sources via Subspace Factor Analysis.” Journal of the American Statistical Association, no. just-accepted: 1–25.\n\n\nDe Vito, Roberta, Ruggero Bellio, Lorenzo Trippa, and Giovanni Parmigiani. 2021. “Bayesian Multistudy Factor Analysis for High-Throughput Biological Data.” The Annals of Applied Statistics 15 (4): 1723–41.\n\n\nGrabski, Isabella N, Roberta De Vito, Lorenzo Trippa, and Giovanni Parmigiani. 2023. “Bayesian Combinatorial MultiStudy Factor Analysis.” The Annals of Applied Statistics 17 (3): 2212.\n\n\nRoy, Arkaprava, Isaac Lavine, Amy H. Herring, and David B. Dunson. 2021. “Perturbed Factor Analysis: Accounting for Group Differences in Exposure Profiles.” The Annals of Applied Statistics 15 (3): 1386–1404.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "Installation.html",
    "href": "Installation.html",
    "title": "1  Package Installation",
    "section": "",
    "text": "1.1 Stack FA, Ind FA, and BMSFA\nThe Stack FA, Ind FA, and BMSFA models are implemented in the MSFA package, which is not available on CRAN. We can install it from GitHub using the remotes package. We use sp_fa() within the MSFA package to fit the Stack FA and Ind FA model, and sp_msfa() to fit the BMSFA model.\ninstall.packages(\"remotes\")\nremotes::install_github(\"rdevito/MSFA\")\nlibrary(MSFA)\nI edited the functions a bit so that sp_fa() and sp_msfa() can accept the scaling and centering arguments. So that we can only center the data without scaling it, or centering and scaling, or neither.\n#devtools::install_github(\"Mavis-Liang/MSFA\")",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Package Installation</span>"
    ]
  },
  {
    "objectID": "Installation.html#pfa",
    "href": "Installation.html#pfa",
    "title": "1  Package Installation",
    "section": "1.2 PFA",
    "text": "1.2 PFA\nPFA does not provide any downloadable R packages and we need to download the R scripts from their GitHub repository, put them in the same directory as the main script, and source them for use.\nWe only need the three files: FBPFA-PFA.R, FBPFA-PFA with fixed latent dim.R, and PFA.cpp, which can be found in https://github.com/royarkaprava/Perturbed-factor-models. The FBPFA-PFA.R file contains the full Bayesian inference algorithm for the PFA model, directly set the latent dimensions equal to the dimensions or the original data. The FBPFA-PFA with fixed latent dim.R file contains the same algorithm that requires to set numbers of common factors \\(K\\). We also notice that two version of the models are both PFA(), and some functions in the FBPFA-PFA with fixed latent dim.R file depends on the FBPFA-PFA.R file. Therefore, since we want to run the dimension reduction version of the model, we must source the FBPFA-PFA.R file first, and then source the FBPFA-PFA with fixed latent dim.R file.\n\n# Suppose the files are in the same directory as the main script\nsource(\"FBPFA-PFA.R\")\nsource(\"FBPFA-PFA with fixed latent dim.R\")",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Package Installation</span>"
    ]
  },
  {
    "objectID": "Installation.html#mom-ss",
    "href": "Installation.html#mom-ss",
    "title": "1  Package Installation",
    "section": "1.3 MOM-SS",
    "text": "1.3 MOM-SS\n\nBiocManager::install(\"sparseMatrixStats\") # Dependency\ninstall.packages(\"mombf\")\n\ninstall.packages(\"devtools\") \n\ndevtools::install_github(\"AleAviP/BFR.BE\")\nlibrary(BFR.BE)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Package Installation</span>"
    ]
  },
  {
    "objectID": "Installation.html#sufa",
    "href": "Installation.html#sufa",
    "title": "1  Package Installation",
    "section": "1.4 SUFA",
    "text": "1.4 SUFA\nTo install SUFA on Linux, you need to also install extra dependencies like PROJ, sqlite3 and GDAL onto PATH. On Windows, you might need to do several updates, particularly the updates for terra. We can skip building the vignettes to save time, as it contains the computation of a large dataset.\n\ndevtools::install_github(\"noirritchandra/SUFA\", build_vignettes = FALSE)\nlibrary(SUFA)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Package Installation</span>"
    ]
  },
  {
    "objectID": "Installation.html#tetris",
    "href": "Installation.html#tetris",
    "title": "1  Package Installation",
    "section": "1.5 Tetris",
    "text": "1.5 Tetris\nSimilar to PFA, Tetris does not provide any downloadable R packages and we need to download the R scripts from their GitHub repository, put them in the same directory as the main script, and source them for use. The R scripts can be found in https://github.com/igrabski/tetris/tree/main.\n\n# Suppose the files are in the same directory as the main script\nsource(\"Tetris.R\")",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Package Installation</span>"
    ]
  },
  {
    "objectID": "Installation.html#other-utility-packages",
    "href": "Installation.html#other-utility-packages",
    "title": "1  Package Installation",
    "section": "1.6 Other utility packages",
    "text": "1.6 Other utility packages\n\nlibrary(tidyverse)\nlibrary(Matrix)#for the bdiag function",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Package Installation</span>"
    ]
  },
  {
    "objectID": "Nutrition.html",
    "href": "Nutrition.html",
    "title": "2  Case study: nutrition data",
    "section": "",
    "text": "2.1 Loading and previewing the data\nThe data used in this section is from a large multi-site study investigating health and diet among Hispanic/Latino adults in the United States (De Vito et al. 2022). This data is not publicly available. Please contact the authors of the original study for access.\nHowever, you can use this simulated data instead: simulated_nutrition_data.rds (if it does not triggers instant download, you can find it in the repo site). To read in the data, use readRDS(). The loaded object contains data in both Y_mat and Y_list formats, as well as covariates and other information. It is generated with the Scenario 4 in the manuscript (see code). Note that instead of having 6 studies, this simulated data has 12 studies.\nload(\"./Data/dataLAT_projale2.rda\")\nThe resulting object is a list of 6 data frames, each corresponding to a different study. Each data frame contains information about the nutritional intake of individuals, and the columns represent different nutrients. From Study 1 to Study 6, the number of individuals (\\(N_s\\)) are 1364, 1517, 2210, 5184, 2478, and 959, respectively, and the number of nutrients (\\(P\\)) are all 42.\n# Check how many studies in the list\nlength(X_s2)\n\n[1] 6\n\n# Dimension of each study\nlapply(X_s2, dim)\n\n[[1]]\n[1] 1364   42\n\n[[2]]\n[1] 1517   42\n\n[[3]]\n[1] 2210   42\n\n[[4]]\n[1] 5184   42\n\n[[5]]\n[1] 2478   42\n\n[[6]]\n[1] 959  42\nLet’s take a look at the first few rows of the first data frame to get an idea of the data structure.\nX_s2[[1]][1:5, 1:5]\n\n  Animal Protein (g) Vegetable Protein (g) Cholesterol (mg)  SCSFA MCSFA\n1            28.9560               14.7440          256.761 0.2665 0.939\n2            33.6675                8.9710          104.217 0.2180 0.520\n3            70.0000               31.0635          207.902 0.9845 1.692\n4            20.6700               13.8240          148.921 0.0625 0.239\n5            15.4250               10.5550           65.060 0.0090 0.033\nWe note that the data we have available is different from the original data (cite). The original data is a collection of 12 studies, and there are known covariates for each individuals, like the one we simulated in the previous section. However, for the purpose of this case study, the data we used are collapsed into 6 studies, and only the nutritional intake data are available.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Case study: nutrition data</span>"
    ]
  },
  {
    "objectID": "Nutrition.html#data-preprocessing",
    "href": "Nutrition.html#data-preprocessing",
    "title": "2  Case study: nutrition data",
    "section": "2.2 Data preprocessing",
    "text": "2.2 Data preprocessing\nSome individuals have missing values for all nutrients, thus we will remove these individuals from the data. Also, there are some nutrition intake are less than zero, for which we will replace with 0. We then apply a log transformation to the data.\nWe first count how many NA values and negative values are in each study.\n\ncount_na_and_negatives &lt;- function(df) {\n  # Count NA values\n  na_count &lt;- sum(is.na(df))\n  # Count negative values\n  negative_count &lt;- sum(df &lt; 0, na.rm = TRUE)\n  \n  # Print counts\n  cat(\"Number of NAs:\", na_count, \"\\n\")\n  cat(\"Number of negative values:\", negative_count, \"\\n\")\n}\ninvisible(lapply(X_s2, count_na_and_negatives))\n\nNumber of NAs: 1344 \nNumber of negative values: 0 \nNumber of NAs: 1344 \nNumber of negative values: 1 \nNumber of NAs: 1344 \nNumber of negative values: 0 \nNumber of NAs: 1344 \nNumber of negative values: 2 \nNumber of NAs: 1344 \nNumber of negative values: 1 \nNumber of NAs: 1344 \nNumber of negative values: 0 \n\n\nWe will define a function to process the data, which removes rows where all values are NA. We also define a function that replaces negative values with 0, and applies a log transformation to the data.\n\nprocess_study_data &lt;- function(df) {\n  # Remove rows where all values are NA\n  cleaned_df &lt;- df[!apply(df, 1, function(row) all(is.na(row))), , drop = FALSE]\n  # Count remaining rows\n  remaining_rows &lt;- nrow(cleaned_df)\n  # Print results for the study\n  cat(\"Remaining rows:\", remaining_rows, \"\\n\")\n  return(cleaned_df)\n}\nY_list &lt;- lapply(X_s2, process_study_data)\n\nRemaining rows: 1332 \nRemaining rows: 1485 \nRemaining rows: 2178 \nRemaining rows: 5152 \nRemaining rows: 2446 \nRemaining rows: 927 \n\n\nThe numbers of individuals in each study left for analysis (\\(N_s\\)) are 1332, 1485, 2178, 5152, 2446, and 927, respectively.\n\n# Replace negative values with 0, then log(x+0.01) + 0.01\nreplace_negatives &lt;- function(df) {\n  # Replace negative values with 0\n  df[df &lt; 0] &lt;- 0\n  # Apply log transformation. Add 0.01 to avoid log(0).\n  transformed_df &lt;- log(df + 0.01)\n  return(transformed_df)\n}\n\nY_list &lt;- lapply(Y_list, replace_negatives)\n\n\n# Check the processed data\ninvisible(lapply(Y_list, count_na_and_negatives))\n\nNumber of NAs: 0 \nNumber of negative values: 11910 \nNumber of NAs: 0 \nNumber of negative values: 11222 \nNumber of NAs: 0 \nNumber of negative values: 15006 \nNumber of NAs: 0 \nNumber of negative values: 36230 \nNumber of NAs: 0 \nNumber of negative values: 19349 \nNumber of NAs: 0 \nNumber of negative values: 6707 \n\n\nNow we don’t have any NA values or negative values in the data.\nThe assumptions for factor models require that each variable has a mean of 0. Therefore, for each study, we will center the data for each column. We note that for some model (Stack FA, Ind FA, BMSFA, and Tetris), this step is handeled internally, and for MOM-SS, the random intercepts are estimated, so we do not need to center the data.\n\nY_list_scaled &lt;- lapply(\n  Y_list, function(x) scale(x, center = TRUE, scale = FALSE)\n)\nY_mat_scaled &lt;- Y_list_scaled %&gt;% do.call(rbind, .) %&gt;% as.matrix()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Case study: nutrition data</span>"
    ]
  },
  {
    "objectID": "Nutrition.html#model-fitting",
    "href": "Nutrition.html#model-fitting",
    "title": "2  Case study: nutrition data",
    "section": "2.3 Model fitting",
    "text": "2.3 Model fitting\nWe recommend running model fitting and post-processing in a high-performance computing environment, as the model fitting process can be computationally intensive. PFA and Tetris are particularly computationally expensive, where PFA requires more than 10 hours to run, and Tetris requires more than 4 days to run. Other models can be completed in half an hour. We recommend at least 5GB of memory for running the models and post-processing.\n\n# Stack FA\nY_mat =  Y_list %&gt;% do.call(rbind, .) %&gt;% as.matrix()\nfit_stackFA &lt;- MSFA::sp_fa(Y_mat, k = 6, scaling = FALSE, centering = TRUE, \n                              control = list(nrun = 10000, burn = 8000))\n# Ind FA\nfit_indFA &lt;-\n      lapply(1:6, function(s){\n        j_s = c(8, 8, 8, 8, 8, 8)\n        MSFA::sp_fa(Y_list[[s]], k = j_s[s], scaling = FALSE, centering = TRUE,\n                    control = list(nrun = 10000, burn = 8000))\n      })\n\n# PFA\nN_s &lt;- sapply(Y_list, nrow)\nfit_PFA &lt;- PFA(Y=t(Y_mat_scaled),\n                        latentdim = 6,\n                        grpind = rep(1:6,\n                                    times = N_s),\n                Thin = 5,\n                Cutoff = 0.001,\n                Total_itr = 10000, burn = 8000)\n\n# MOM-SS\nY_mat =  Y_list %&gt;% do.call(rbind, .) %&gt;% as.matrix()\n# Construct the membership matrix\nN_s &lt;- sapply(Y_list, nrow)\nM_list &lt;- list()\n   for(s in 1:6){\n     M_list[[s]] &lt;- matrix(1, nrow = N_s[s], ncol = 1)\n   }\nM &lt;- as.matrix(bdiag(M_list))\nfit_MOMSS &lt;- BFR.BE::BFR.BE.EM.CV(x = Y_mat, v = NULL, \n                                  b = M, q = 6, scaling = FALSE)\n\n\n# SUFA\nfit_SUFA &lt;- SUFA::fit_SUFA(Y_list_scaled, qmax=6, nrun = 10000)\n\n# BMSFA\nfit_BMSFA &lt;- MSFA::sp_msfa(Y_list, k = 6, j_s = c(2, 2, 2, 2, 2, 2),\n                           outputlevel = 1, scaling = FALSE, \n                           centering = TRUE,\n                           control = list(nrun = 10000, burn = 8000))\n\nFitting Tetris requires a 3-step process. First, we run tetris() to draw posterior samples of the model parameters, including \\(\\mathcal{T}\\). Then we run choose.A() to choose the best \\(\\mathcal{T}\\) based on the posterior samples. Finally, we run tetris() again with the chosen \\(\\mathcal{T}\\) to obtain the final model. Hyperparameters \\(\\alpha_{\\mathcal{T}}\\) are set to 1.25 times the number of studies.\n\n# Tetris\nset_alpha &lt;- ceiling(1.25*6)\nfit_Tetris &lt;- tetris(Y_list, alpha=set_alpha, beta=1, nprint = 200, \n                     nrun=10000, burn=8000)\nbig_T &lt;- choose.A(fit_Tetris, alpha_IBP=set_alpha, S=6)\nrun_fixed &lt;- tetris(Y_list, alpha=set_alpha, beta=1, \n                    fixed=TRUE, A_fixed=big_T, nprint = 200, \n                    nrun=10000, burn=8000)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Case study: nutrition data</span>"
    ]
  },
  {
    "objectID": "Nutrition.html#post-processing",
    "href": "Nutrition.html#post-processing",
    "title": "2  Case study: nutrition data",
    "section": "2.4 Post processing",
    "text": "2.4 Post processing\nPost processing includes calculating the point estimates of the factor loadings and covariance matrix from the posterior samples, and determing the number of factors for each model.\nFor methods of MOM-SS, SUFA, and Tetris, the number of factors is determined internally in the algorithms, therefore, the output of the models are final results.\nFor MOM-SS, \\(\\Phi\\) is directly obtained with its post-processed common loadings in the fitted output. Th common covariance is calculated with \\(\\Phi\\Phi^\\top\\). The marginal covariance matrix \\(\\Sigma_{\\text{marginal}}\\) is calculated by adding the estimated study-specific error covariances to the common variance. The study-specific intercepts \\(\\alpha\\) and the coefficients for the known covariates \\(B\\) are also extracted from the fitted output.\n\npost_MOMSS &lt;- function(fit, version = 2){ # version 1: M, version 2: Mpost\n  est_Phi &lt;- fit$M\n  if (version==2){est_Phi &lt;- fit$Mpost}\n  est_SigmaPhi &lt;- tcrossprod(est_Phi)\n  \n  # Marginal covariance\n  S &lt;- dim(fit$sigma)[2]\n  est_PsiList &lt;- est_SigmaMarginal &lt;-  list()\n  for(s in 1:S){\n    est_PsiList[[s]] &lt;- fit$sigma[,s]\n    est_SigmaMarginal[[s]] &lt;- est_SigmaPhi + diag(fit$sigma[,s])\n  }\n  # last S columns of fit$Theta are the study-specific intercepts\n  est_alphas &lt;- fit$Theta[, (dim(fit$Theta)[2]-S+1):dim(fit$Theta)[2]]\n  # The rest are coeficients for the known covariates\n  est_B &lt;- fit$Theta[, 1:(dim(fit$Theta)[2]-S)]\n  \n  return(list(Phi = est_Phi, SigmaPhi = est_SigmaPhi, Psi = est_PsiList, alpha = est_alphas, B = est_B,\n              SigmaMarginal = est_SigmaMarginal))\n}\nres_MOMSS &lt;- post_MOMSS(fit_MOMSS)\nsaveRDS(res_MOMSS, \"Data/Rnutrition_MOMSS.rds\")\n\nFor SUFA, the shared and study-specific loading matrices, as well as the common and marginal covariance can be conveniently obtained via the lam.est.all(), SUFA_shared_covmat() and sufa_marginal_covs() functions. Error covariance is obtained by taking averages of the “residuals” fitted output. And the study-specific covariance matrices are calculated by subtracting the common covariance from the marginal covariance. Note that in the definition of SUFA, common covariance is \\(\\Phi\\Phi^\\top + \\Sigma\\).\n\npost_SUFA &lt;- function(fit){\n  all &lt;- dim(fit$Lambda)[3]\n  burnin &lt;- floor(all * 0.8) # We will use the last 20% samples\n  # shared and study-specific loading matrices\n  loadings &lt;- lam.est.all(fit, burn = burnin)\n  # Obtain common covariance matrix and loading from fitting\n  est_Phi &lt;- loadings$Shared\n  est_SigmaPhi &lt;- SUFA_shared_covmat(fit, burn = burnin)\n  est_Psi &lt;- diag(colMeans(fit$residuals))\n  # Study-specific loadings\n  est_LambdaList &lt;- loadings$Study_specific\n  \n  # Obtain study-specific covariance matrices\n  S &lt;- length(fit$A)\n  marginal_cov &lt;- sufa_marginal_covs(fit, burn = burnin)\n  est_SigmaLambdaList &lt;- list()\n  for (s in 1:S) {\n    est_SigmaLambdaList[[s]] &lt;- marginal_cov[,,s] - est_SigmaPhi\n  }\n  \n  return(list(SigmaPhi = est_SigmaPhi, Phi = est_Phi, \n              SigmaLambdaList = est_SigmaLambdaList,\n              LambdaList = est_LambdaList, \n              Psi = est_Psi,\n              SigmaMarginal = lapply(1:S, function(s) marginal_cov[,,s])\n              ))\n}\nres_SUFA &lt;- post_SUFA(fit_SUFA)\nsaveRDS(res_SUFA, \"Data/Rnutrition_SUFA.rds\")\n\nFor Tetris, the common loading matrix \\(\\Phi\\) can be obtained through the getLambda() function. The common covariance matrix is calculated as \\(\\Phi\\Phi^\\top\\). The study-specific loading matrices \\(\\Lambda_s\\) are obtained by multiplying the common loading matrix with the study-specific matrices \\(T_s - P\\). The study-specific covariance matrices are calculated as \\(\\Lambda_s\\Lambda_s^\\top\\). The marginal covariance matrix is calculated as \\(\\Lambda T \\Lambda^\\top + \\Psi\\).\n\npost_Tetris &lt;- function(fit){\n  # Estimated common covariance\n  A &lt;- fit$A[[1]]\n  Lambda &lt;- getLambda(fit,A)\n  S &lt;- dim(A)[1]\n  est_Phi &lt;- as.matrix(Lambda[,colSums(A)==S])\n  est_SigmaPhi &lt;- tcrossprod(est_Phi)\n  # Estimated study-specific covariance\n  P = diag((colSums(A) == S)*1)\n  T_s &lt;- list()\n  est_LambdaList &lt;- list()\n  for(s in 1:S){\n    T_s[[s]] &lt;- diag(A[s,])\n    Lambda_s &lt;- Lambda %*% (T_s[[s]] - P)\n    Lambda_s &lt;- Lambda_s[,-which(colSums(Lambda_s == 0) == nrow(Lambda_s))]\n    Lambda_s &lt;- matrix(Lambda_s, nrow=nrow(Lambda))\n    est_LambdaList[[s]] &lt;- Lambda_s}\n  est_SigmaLambdaList &lt;- lapply(1:S, function(s){\n    tcrossprod(est_LambdaList[[s]])})\n  \n  # Estimated marginal covariance\n  Psi &lt;- list()\n  est_SigmaMarginal &lt;- lapply(1:S, function(s){\n    Psi[[s]] &lt;- diag(Reduce(\"+\", fit$Psi[[s]])/length(fit$Psi[[s]]))\n    Sigma_s &lt;- Lambda %*% T_s[[s]] %*% t(Lambda) + Psi[[s]]\n    })\n  \n  return(list(Phi = est_Phi, SigmaPhi = est_SigmaPhi,\n              LambdaList = est_LambdaList, SigmaLambdaList = est_SigmaLambdaList,\n              Psi = Psi, T_s = T_s,\n              SigmaMarginal = est_SigmaMarginal))\n}\nres_Tetris &lt;- post_Tetris(run_fixed)\nsaveRDS(res_Tetris, \"Data/Rnutrition_Tetris.rds\")\n\nThe following code shows the post-processing for PFA.First, we post-process for the number of factors. We first extracts the number of factors \\(K\\) for each posterior sample, by calculating the number of columns in the loadings matrix. Then we finds the mode \\(K\\) of these values, and filter posterior samples to only those with the modal \\(K\\). We continue with the downstream summary using only those aligned samples. To get the common loadings by its definition, we first multiply \\(\\Phi\\) with \\(V^{1/2}\\) in the samples we kept. While the common loading matrix \\(\\Phi\\) can be simply calculated by the average of the posterior \\(\\Phi V^{1/2}\\), with its adjustment to address identifiability issues, we still apply OP to \\(\\Phi V^{1/2}\\). For the covariances such as the common covariance, we calculate \\(\\Phi V\\Phi^\\top\\) at each iterations, and then use their average as the final results. Similar procedure is applied for the study-specific covariances and marginal covariance.\n\npost_PFA &lt;- function(fit) {\n  # Determine posterior dimension (number of factors per sample)\n  k_vec &lt;- sapply(fit$Loading, ncol)\n  mode_k &lt;- as.numeric(names(sort(table(k_vec), decreasing = TRUE)[1]))\n  \n  # Filter posterior samples to those with mode_k\n  keep_idx &lt;- which(k_vec == mode_k)\n  fit$Loading &lt;- fit$Loading[keep_idx]\n  fit$Latentsigma &lt;- fit$Latentsigma[keep_idx]\n  fit$Errorsigma &lt;- fit$Errorsigma[keep_idx]\n  fit$Pertmat &lt;- fit$Pertmat[keep_idx]\n  \n  npost &lt;- length(fit$Loading)\n  p &lt;- nrow(fit$Loading[[1]])\n  k &lt;- mode_k\n  S &lt;- dim(fit$Pertmat[[1]])[2]\n  \n  posteriorPhis &lt;- array(0, dim = c(p, k, npost))\n  posteriorLams &lt;- vector(\"list\", S)\n\n  for(s in 1:S){\n    posteriorLams[[s]] &lt;- array(0, dim = c(p, k, npost))\n    for(i in 1:npost){\n      posteriorPhis[,,i] &lt;- fit$Loading[[i]] %*% diag(fit$Latentsigma[[i]])\n      posteriorLams[[s]][,,i] &lt;- (solve(matrix(fit$Pertmat[[i]][, s], p, p)) - diag(p)) %*% posteriorPhis[,,i]\n    }\n  }\n\n  # Varimax rotation\n  est_Phi &lt;- MSFA::sp_OP(posteriorPhis, itermax = 10, trace = FALSE)$Phi\n  est_speLoad &lt;- lapply(posteriorLams, function(x) MSFA::sp_OP(x, itermax = 10, trace = FALSE)$Phi)\n\n  # Estimated covariance components\n  sharevar &lt;- list()\n  est_SigmaLambdaList &lt;- vector(\"list\", S)\n  est_SigmaMarginal &lt;- vector(\"list\", S)\n  est_Psi_list &lt;- list()\n\n  for(s in 1:S){\n    post_SigmaLambda_s &lt;- vector(\"list\", npost)\n    post_SigmaMarginal_s &lt;- vector(\"list\", npost)\n    Psi &lt;- vector(\"list\", npost)\n\n    for(i in 1:npost){\n      sharevar[[i]] &lt;- fit$Loading[[i]] %*% diag(fit$Latentsigma[[i]]^2) %*% t(fit$Loading[[i]]) + \n        diag(fit$Errorsigma[[i]]^2)\n      Q_temp_inv &lt;- solve(matrix(fit$Pertmat[[i]][, s], p, p))\n      post_SigmaMarginal_s[[i]] &lt;- Q_temp_inv %*% sharevar[[i]] %*% t(Q_temp_inv)\n      post_SigmaLambda_s[[i]] &lt;- post_SigmaMarginal_s[[i]] - sharevar[[i]]\n      Psi[[i]] &lt;- diag(fit$Errorsigma[[i]]^2)\n    }\n\n    est_SigmaMarginal[[s]] &lt;- Reduce('+', post_SigmaMarginal_s) / npost\n    est_SigmaLambdaList[[s]] &lt;- Reduce('+', post_SigmaLambda_s) / npost\n    est_Psi_list[[s]] &lt;- Reduce('+', Psi) / npost\n  }\n\n  est_Psi &lt;- Reduce('+', est_Psi_list) / S\n  est_SigmaPhi &lt;- Reduce('+', sharevar) / npost\n  est_Q &lt;- Reduce('+', fit$Pertmat) / npost\n  est_Q_list &lt;- lapply(1:S, function(s) matrix(est_Q[, s], p, p))\n\n  return(list(\n    Phi = est_Phi,\n    SigmaPhi = est_SigmaPhi,\n    Psi = est_Psi,\n    Q = est_Q_list,\n    LambdaList = est_speLoad,\n    SigmaLambdaList = est_SigmaLambdaList,\n    SigmaMarginal = est_SigmaMarginal,\n    mode_k = mode_k,\n    kept_samples = length(keep_idx)\n  ))\n}\nres_PFA &lt;- post_PFA(fit_PFA)\nsaveRDS(res_PFA, \"Data/Rnutrition_PFA.rds\")\n\n\n# columns have all loadings less than 10^-3\nfun_neibour &lt;- function(Phi, threshold = 1e-3) {\n  return(\n    sum(apply(Phi, 2, function(x) {\n      sum(abs(x) &lt;= threshold) &lt; length(x)}\n    ))\n  )\n}\n\n# The post_PFA(fit_PFA) object\nres_PFA &lt;- readRDS(\"Data/Rnutrition_PFA.rds\")\nPhi_PFA &lt;- res_PFA$Phi\nK_PFA &lt;- fun_neibour(Phi_PFA)\nK_PFA\n\n[1] 6\n\n\nThe estimated \\(K\\) for PFA is 6.\nNext, we show how to process the output of Stack FA, Ind FA, and BMSFA. While point estimates for the loadings and covariances can be obtained in the similar way as in PFA, the number of factors is determined with eigen value decompositions of the covariance matrix. Once the numbers of factors are determined, we have to run the models again with the correct number of factors, and then extract the final results.\nTherefore, for Stack FA, we first extract the point estimates of the \\(\\Phi\\) by applying OP to the posterior samples of the loadings. The common covariance matrix is calculated as \\(\\Phi\\Phi^\\top\\). The marginal covariance matrix is calculated as the average of its posterior samples.\n\npost_stackFA &lt;- function(fit, S){\n  est_Phi &lt;- MSFA::sp_OP(fit$Lambda, trace=FALSE)$Phi\n  est_SigmaPhi &lt;- tcrossprod(est_Phi)\n  est_SigmaMarginal &lt;-  lapply(1:S, function(s)\n    apply(fit$Sigma, c(1, 2), mean)\n  )\n  Psi_chain &lt;- list()\n  for(i in 1:dim(fit$Sigma)[3]){\n    Psi_chain[[i]] &lt;- fit$Sigma[, , i] - tcrossprod(fit$Lambda[, , i])\n  }\n  est_Psi &lt;- Reduce('+', Psi_chain)/length(Psi_chain)\n  return(list(Phi = est_Phi, SigmaPhi = est_SigmaPhi, Psi = est_Psi,\n              SigmaMarginal = est_SigmaMarginal))\n}\nres_stackFA &lt;- post_stackFA(fit_stackFA, S=6)\nsaveRDS(res_stackFA, \"Data/Rnutrition_StackFA.rds\")\n\nAfter that, we determine the number of factors by eigen value decomposition of the common covariance matrix. We then run the model again with the correct number of factors, and extract the final results.\n\nfun_eigen &lt;- function(Sig_mean) {\n  val_eigen &lt;- eigen(Sig_mean)$values\n  prop_var &lt;- val_eigen/sum(val_eigen)\n  choose_K &lt;- length(which(prop_var &gt; 0.05))\n  return(choose_K)\n}\nres_stackFA &lt;- readRDS(\"Data/Rnutrition_StackFA.rds\")\nSigmaPhi_StackFA &lt;- res_stackFA$SigmaPhi \nK_StackFA &lt;- fun_eigen(SigmaPhi_StackFA)\nK_StackFA\n\n[1] 4\n\n\nThe estimated \\(K\\) for Stack FA is 4.\nThen we re-run the model with the correct number of factors, and extract the final results.\n\nfit_stackFA_2 &lt;- MSFA::sp_fa(Y_mat_scaled, k = K_StackFA, scaling = FALSE, centering = TRUE, \n                           control = list(nrun = 10000, burn = 8000))\nres_stackFA_2 &lt;- post_stackFA(fit_stackFA_2, S=6)\nsaveRDS(res_stackFA_2, \"Data/Rnutrition_StackFA_2.rds\")\n\nWe repeat this process for Ind FA and BMSFA.\n\n# Ind FA\npost_indFA &lt;- function(fit){\n  # Estimated study-specific covariance and loading\n  S &lt;- length(fit_list)\n  est_LambdaList &lt;- lapply(1:S, function(s){\n    MSFA::sp_OP(fit_list[[s]]$Lambda, trace=FALSE)$Phi\n  })\n  est_SigmaLambdaList &lt;- lapply(est_LambdaList, function(x) tcrossprod(x))\n  \n  # Marginal covariance matrices\n  est_SigmaMarginal &lt;- lapply(1:S, function(s) {\n    fit &lt;- fit_list[[s]]\n    apply(fit$Sigma, c(1, 2), mean)\n  })\n\n  Psi &lt;- list()\n  for(s in 1:S){\n    Psi_chain &lt;- list()\n    for(i in 1:dim(fit_list[[1]]$Sigma)[3]){\n      Psi_chain[[i]] &lt;- fit_list[[s]]$Sigma[, , i] - tcrossprod(fit_list[[s]]$Lambda[, , i])\n    }\n    Psi[[s]] &lt;- Reduce('+', Psi_chain)/length(Psi_chain)\n  }\n  \n  return(list(LambdaList = est_LambdaList, SigmaLambdaList = est_SigmaLambdaList, Psi = Psi,\n              SigmaMarginal = est_SigmaMarginal))\n}\n\nres_indFA &lt;- post_indFA(fit_indFA)\nsaveRDS(res_indFA, \"Data/Rnutrition_IndFA.rds\")\n\n# BMSFA\npost_BMSFA &lt;- function(fit){\n  # Common covariance matrix and loading\n  est_Phi &lt;- sp_OP(fit$Phi, trace=FALSE)$Phi\n  est_SigmaPhi &lt;- tcrossprod(est_Phi)\n  \n  # Study-specific covariance matrices and loadings\n  est_LambdaList &lt;- lapply(fit$Lambda, function(x) sp_OP(x, trace=FALSE)$Phi)\n  est_SigmaLambdaList &lt;- lapply(est_LambdaList, function(x) tcrossprod(x))\n  \n  # Marginal covariance matrices\n  S &lt;- length(est_SigmaLambdaList)\n  # Get point estimate of each Psi_s\n  est_PsiList &lt;- lapply(1:S, function(s) {\n    apply(fit$psi[[s]], c(1, 2), mean)\n  })\n  est_margin_cov &lt;- lapply(1:S, function(s) {\n    est_SigmaPhi + est_SigmaLambdaList[[s]] + diag(est_PsiList[[s]] %&gt;% as.vector())\n  })\n  \n  return(list(Phi = est_Phi, SigmaPhi = est_SigmaPhi,\n         LambdaList = est_LambdaList, SigmaLambdaList = est_SigmaLambdaList,\n         PsiList = est_PsiList,\n         SigmaMarginal = est_margin_cov))\n}\nres_BMSFA &lt;- post_BMSFA(fit_BMSFA)\nsaveRDS(res_BMSFA, \"Data/Rnutrition_BMSFA.rds\")\n\n\nSigmaLambda_IndFA &lt;- readRDS(\"Data/Rnutrition_IndFA.rds\")$SigmaLambda\nJs_IndFA &lt;- lapply(SigmaLambda_IndFA, fun_eigen)\nJs_IndFA %&gt;% unlist()\n\n[1] 4 5 5 5 4 4\n\nSigmaPhi_BMSFA &lt;- readRDS(\"Data/Rnutrition_BMSFA.rds\")$SigmaPhi\nK_BMSFA &lt;- fun_eigen(SigmaPhi_BMSFA)\nSigmaLambda_BMSFA &lt;- readRDS(\"Data/Rnutrition_BMSFA.rds\")$SigmaLambda\nJs_BMSFA &lt;- lapply(SigmaLambda_BMSFA, fun_eigen)\nK_BMSFA %&gt;% unlist()\n\n[1] 4\n\nJs_BMSFA %&gt;% unlist()\n\n[1] 2 2 2 2 2 2\n\n\nThe estimated \\(J_s\\) for Ind FA are 4, 5, 5, 5, 4, and 4. The estimated \\(K\\) for BMSFA is 4 and the estimated \\(J_s\\) are 2, 2, 2, 2, 2, and 2\nThen we re-run the models with the correct number of factors, and extract the final results.\n\n# Ind FA\nfit_indFA_2 &lt;-\n  lapply(1:6, function(s){\n    j_s = c(4, 5, 5, 5, 4, 4)\n    MSFA::sp_fa(Y_list[[s]], k = j_s[s], scaling = FALSE, centering = TRUE,\n                control = list(nrun = 10000, burn = 8000))\n  })\nres_indFA_2 &lt;- post_indFA(fit_indFA_2)\nsaveRDS(res_indFA_2, \"Data/Rnutrition_IndFA_2.rds\")\n\n# BMSFA\nfit_BMSFA_2 &lt;- MSFA::sp_msfa(Y_list, k = 4, j_s = c(2, 2, 2, 2, 2, 2),\n                           outputlevel = 1, scaling = FALSE, \n                           centering = TRUE,\n                           control = list(nrun = 10000, burn = 8000))\nres_BMSFA_2 &lt;- post_BMSFA(fit_BMSFA_2)\nsaveRDS(res_BMSFA_2, \"Data/Rnutrition_BMSFA_2.rds\")\n\nNow the final results are obtained and you can get the saved files mentioned above from the GitHub repository.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Case study: nutrition data</span>"
    ]
  },
  {
    "objectID": "Nutrition.html#visualization",
    "href": "Nutrition.html#visualization",
    "title": "2  Case study: nutrition data",
    "section": "2.5 Visualization",
    "text": "2.5 Visualization\nWe can make some heatmap for the loadings. Please see the figures in the paper.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Case study: nutrition data</span>"
    ]
  },
  {
    "objectID": "Nutrition.html#mean-squared-error-mse",
    "href": "Nutrition.html#mean-squared-error-mse",
    "title": "2  Case study: nutrition data",
    "section": "2.6 Mean squared error (MSE)",
    "text": "2.6 Mean squared error (MSE)\nWe can assess the goodness-of-fit of the models by reconstructing data and calculating the reconstruction errors. With estimated loadings and error covariances, we can estimate the factor scores for a new observation, \\(\\widehat{\\mathbf{f}}_{is,(new)}\\) and \\(\\widehat{\\mathbf{l}}_{is,(new)}\\), derived by adapting the Bartlett method. Then we can use the estimated factor scores, together with the estimated loadings, to reconstruct \\(\\widehat{\\mathbf{y}}_{is,(new)}\\) and calculate the reconstruction error which represents the fit of the models. To be specific, suppose that we have the multivariate data of a new observation from a specific study, \\(\\mathbf{y}_{is,(new)}\\), we can estimate its factor score and reconstruct its \\(\\widehat{\\mathbf{y}}_{is,(new)}\\) in the following ways:\nIf we divide the whole data into training set and test set, we can obtain the between the true and estimated \\(\\mathbf{y}_{is,(new)}\\) in the test set for all individuals in all studies with \\(\\frac{1}{P\\sum _s^S N_s}\\sum_{s}^S\\sum_{i}^{N_s}\\sum_{p}^P(\\widehat{y}_{isp,(new)}-y_{isp,(new)})^2\\).\nIn the following, we divide the nutrition data into training set (70%) and test set (30%) and calculate the MSE of each model. Note that \\(\\mathbf{y}_{new}\\) should also be centered except for MOM-SS.\n\ntrain_ratio &lt;- 0.7\ntrain_list &lt;- list()\ntest_list &lt;- list()\n\nfor (s in seq_along(Y_list)) {\n  N_s &lt;- nrow(Y_list[[s]])  # Number of rows in the study\n  train_indices &lt;- sample(1:N_s, size = floor(train_ratio * N_s), replace = FALSE)\n  \n  train_list[[s]] &lt;- Y_list[[s]][train_indices, ]\n  test_list[[s]] &lt;- Y_list[[s]][-train_indices, ]\n}\n# Test data has to be centered\ntest_list &lt;- lapply(test_list, as.matrix)\ntest_list_scaled &lt;- lapply(\n  test_list, function(x) scale(x, center = TRUE, scale = FALSE)\n)\n\nWe fit the models in the training data (on high-performance computing clusters), and then we load the fitted models and calculate the MSE on the test set for each model. The numbers of factors we input are determined by the previous results.\n\nfit_StackFA_train &lt;- readRDS(\"Data/Rnutrition_stackFA_train.rds\")\nfit_IndFA_train &lt;- readRDS(\"Data/Rnutrition_IndFA_train.rds\")\nfit_PFA_train &lt;- readRDS(\"Data/Rnutrition_PFA_train.rds\")\nfit_MOMSS_train &lt;- readRDS(\"Data/Rnutrition_MOMSS_train.rds\")\nfit_SUFA_train &lt;- readRDS(\"Data/Rnutrition_SUFA_train.rds\")\nfit_BMSFA_train &lt;- readRDS(\"Data/Rnutrition_BMSFA_train.rds\")\nfit_Tetris_train &lt;- readRDS(\"Data/Rnutrition_Tetris_train.rds\")\n\nWe used the derived MSE function to calculate the MSE for each model.\n\n# Stack FA\nPhi &lt;- fit_StackFA_train$Phi\nPsi &lt;- fit_StackFA_train$Psi\n\nmse_stackFA &lt;- 1/(42 * sum(sapply(test_list, nrow)))*sum(\n  sapply(1:6, function(s){\n    scores &lt;- test_list_scaled[[s]] %*% solve(Psi) %*% Phi %*% mnormt::pd.solve(signif(t(Phi) %*% solve(Psi) %*% Phi))\n    norm(test_list_scaled[[s]] - scores %*% t(Phi), \"F\")^2\n  })\n)\n\n\n# Ind FA\nLambdaList &lt;- fit_IndFA_train$LambdaList\nPsi &lt;- fit_IndFA_train$Psi\nmse_IndFA &lt;- 1/(42 * sum(sapply(test_list, nrow)))*sum(\n  sapply(1:6, function(s){\n    scores &lt;- test_list_scaled[[s]] %*% solve(Psi[[s]]) %*% LambdaList[[s]] %*% mnormt::pd.solve(signif(t(LambdaList[[s]]) %*% solve(Psi[[s]]) %*% LambdaList[[s]]))\n    \n    norm(test_list_scaled[[s]] - scores %*% t(LambdaList[[s]]), \"F\")^2\n  })\n)\n\n\n# PFA\nPhi &lt;- fit_PFA_train$Phi\nPsi &lt;- fit_PFA_train$Psi\nQ_list &lt;- fit_PFA_train$Q\nmse_PFA &lt;- 1/(42 * sum(sapply(test_list, nrow)))*sum(\n  sapply(1:6, function(s){\n    scores &lt;- test_list_scaled[[s]] %*% t(Q_list[[s]]) %*% Phi %*% mnormt::pd.solve(signif(t(Phi) %*% solve(Psi) %*% Phi))\n    Y_est &lt;- scores %*% t(Phi) %*% t(solve(Q_list[[s]]))\n    norm(test_list_scaled[[s]] - Y_est, \"F\")^2\n  })\n)\n\n\n# MOM-SS\nPhi &lt;- fit_MOMSS_train$Phi\nPsi &lt;- fit_MOMSS_train$Psi %&gt;% lapply(diag)\nalpha &lt;- lapply(1:6, function(s) {\n  fit_MOMSS_train$alpha[,s]\n})\n\nmse_MOMSS &lt;- 1/(42 * sum(sapply(test_list, nrow)))*sum(\n  sapply(1:6, function(s){\n    scores &lt;- t(apply(test_list[[s]], 1, function(row) {row - alpha[[s]]})) %*% solve(Psi[[s]]) %*% Phi %*% mnormt::pd.solve(signif(t(Phi) %*% solve(Psi[[s]]) %*% Phi))\n    Y_est &lt;- t(apply(scores %*% t(Phi), 1, function(row) {row + alpha[[s]]}))\n    norm(test_list[[s]] - Y_est, \"F\")^2\n  })\n)\n\n\n# SUFA\nPhi &lt;- fit_SUFA_train$Phi\nLambdaList &lt;- fit_SUFA_train$LambdaList\nPsi &lt;- fit_SUFA_train$Psi\nmse_SUFA &lt;- 1/(42 * sum(sapply(test_list, nrow)))*sum(\n  sapply(1:6, function(s){\n    Omega &lt;- cbind(Phi, LambdaList[[s]])\n    scores &lt;- test_list_scaled[[s]] %*% solve(Psi) %*% Omega %*% mnormt::pd.solve(signif(t(Omega) %*% solve(Psi) %*% Omega))\n    \n    norm(test_list_scaled[[s]] - scores %*% t(Omega), \"F\")^2\n  })\n)\n\n\n# BMSFA\nPhi &lt;- fit_BMSFA_train$Phi\nLambdaList &lt;- fit_BMSFA_train$LambdaList\nPsi &lt;- fit_BMSFA_train$PsiList %&gt;%  lapply(as.vector) %&gt;% lapply(diag)\nmse_BMSFA &lt;- 1/(42 * sum(sapply(test_list, nrow)))*sum(\n  sapply(1:6, function(s){\n    Omega &lt;- cbind(Phi, LambdaList[[s]])\n    scores &lt;- test_list_scaled[[s]] %*% solve(Psi[[s]]) %*% Omega %*% mnormt::pd.solve(signif(t(Omega) %*% solve(Psi[[s]]) %*% Omega))\n    \n    norm(test_list_scaled[[s]] - scores %*% t(Omega), \"F\")^2\n  })\n)\n\n\n# Tetris\nPhi &lt;- fit_Tetris_train$Phi\nLambdaList &lt;- fit_Tetris_train$LambdaList\nMarginal &lt;- fit_Tetris_train$SigmaMarginal\nSigmaPhi &lt;- fit_Tetris_train$SigmaPhi\nSigmaLambdaList &lt;- fit_Tetris_train$SigmaLambdaList\nPsi &lt;- lapply(1:6, function(s) {\n  Marginal[[s]] - SigmaPhi - SigmaLambdaList[[s]]\n})\nT_s_list &lt;- fit_Tetris_train$T_s\nmse_Tetris &lt;- 1/(42 * sum(sapply(test_list, nrow)))*sum(\n  sapply(1:6, function(s){\n    Omega &lt;- cbind(Phi, LambdaList[[s]])\n    scores &lt;- test_list_scaled[[s]] %*% solve(Psi[[s]]) %*% Omega %*% mnormt::pd.solve(signif(t(Omega) %*% solve(Psi[[s]]) %*% Omega))\n    \n    norm(test_list_scaled[[s]] - scores %*% t(Omega), \"F\")^2\n  })\n)\n\nWe display the MSE of each model.\n\nprint(paste0(\"Stack FA: \", mse_stackFA %&gt;% round(3)))\nprint(paste0(\"Ind FA: \", mse_IndFA %&gt;% round(3)))\nprint(paste0(\"PFA: \", mse_PFA %&gt;% round(3)))\nprint(paste0(\"MOM-SS: \", mse_MOMSS %&gt;% round(3)))\nprint(paste0(\"SUFA: \", mse_SUFA %&gt;% round(3)))\nprint(paste0(\"BMSFA: \", mse_BMSFA %&gt;% round(3)))\nprint(paste0(\"Tetris: \", mse_Tetris %&gt;% round(3)))\n\n[1] \"Stack FA: 0.502\"\n[1] \"Ind FA: 0.485\"\n[1] \"PFA: 0.683\"\n[1] \"MOM-SS: 0.474\"\n[1] \"SUFA: 0.437\"\n[1] \"BMSFA: 0.456\"\n[1] \"Tetris: 0.287\"\n\n\n\n\n\n\nDe Vito, Roberta, Briana Stephenson, Daniela Sotres-Alvarez, Anna-Maria Siega-Riz, Josiemer Mattei, Maria Parpinel, Brandilyn A Peters, et al. 2022. “Shared and Ethnic Background Site-Specific Dietary Patterns in the Hispanic Community Health Study/Study of Latinos (HCHS/SOL).” medRxiv, 2022–06.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Case study: nutrition data</span>"
    ]
  },
  {
    "objectID": "Gene.html",
    "href": "Gene.html",
    "title": "3  Case study: gene expression data",
    "section": "",
    "text": "3.1 Loading and previewing the data\nWe load the curatedOvarianData package to get the data. Description of each study can be found with data(package=\"curatedOvarianData\").\nlibrary(curatedOvarianData)\n#data(package=\"curatedOvarianData\")\ndata(GSE13876_eset)\ndata(GSE26712_eset)\ndata(GSE9891_eset)\ndata(PMID17290060_eset)\nThe four datasets are of similar sizes. All of them have the majority of patients in the late stage of the cancer, and histological subtypes observed in the tissue samples are mostly serous carcinoma. The datasets differs in respect sequencing platforms, which are Operonv3two-color, AffymetrixHG-U133A, AffymetrixHG-U133Plus2 and AffymetrixHG-U133A.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Case study: gene expression data</span>"
    ]
  },
  {
    "objectID": "Gene.html#data-pre-processing",
    "href": "Gene.html#data-pre-processing",
    "title": "3  Case study: gene expression data",
    "section": "3.2 Data pre-processing",
    "text": "3.2 Data pre-processing\nFirst we find intersection of genes that are exist in all four studies. We use featureNames to extract the gene names and exprs to extract the data matrices. More operations of the ExpressionSets can be found in (Falcon, Morgan, and Gentleman 2007).\n\ninter_genes &lt;- Reduce(intersect, list(featureNames(GSE13876_eset),\n                                      featureNames(GSE26712_eset), \n                                      featureNames(GSE9891_eset),\n                                      featureNames(PMID17290060_eset)))\n\nGSE13876_eset &lt;- GSE13876_eset[inter_genes,]\nGSE26712_eset &lt;- GSE26712_eset[inter_genes,]\nGSE9891_eset &lt;- GSE9891_eset[inter_genes,]\nPMID17290060_eset &lt;- PMID17290060_eset[inter_genes,]\n\nstudy1 &lt;- t(exprs(GSE13876_eset))\nstudy2 &lt;- t(exprs(GSE26712_eset))\nstudy3 &lt;- t(exprs(GSE9891_eset))\nstudy4 &lt;- t(exprs(PMID17290060_eset))\n\nWe then filter the genes so that only high variance genes are kept for later analysis.\n\n# calculate Coefficient of Variation of each gene\ncv1 &lt;- apply(study1, 2, sd) / apply(study1, 2, mean)\ncv2 &lt;- apply(study2, 2, sd) / apply(study2, 2, mean)\ncv3 &lt;- apply(study3, 2, sd) / apply(study3, 2, mean)\ncv4 &lt;- apply(study4, 2, sd) / apply(study4, 2, mean)\ncv_matrix &lt;- rbind(cv1, cv2, cv3, cv4)\n\n# Find genes with CV &gt;= threshold in at least one study\nthreshold &lt;- 0.16\ngenes_to_keep &lt;- apply(cv_matrix, 2, function(cv) any(cv &gt;= threshold))\nsum(genes_to_keep)\n\n[1] 1060\n\n# Filtered\nstudy1 &lt;- GSE13876_eset[genes_to_keep,]\nstudy2 &lt;- GSE26712_eset[genes_to_keep,]\nstudy3 &lt;- GSE9891_eset[genes_to_keep,]\nstudy4 &lt;- PMID17290060_eset[genes_to_keep,]\n\nNext, we log-transform the data (for a better normality) and store them in 4 \\(N_s\\times P\\) matrices.\n\ndf1 &lt;- study1 %&gt;% exprs() %&gt;% t() %&gt;% \n  log() %&gt;% as.data.frame()\ndf2 &lt;- study2 %&gt;% exprs() %&gt;% t() %&gt;% \n  log() %&gt;% as.data.frame()\ndf3 &lt;- study3 %&gt;% exprs() %&gt;% t() %&gt;% \n  log()  %&gt;% as.data.frame()\ndf4 &lt;- study4 %&gt;% exprs() %&gt;% t() %&gt;% \n  log()  %&gt;% as.data.frame()\nlist_gene &lt;- list(df1, df2, df3, df4)\nsaveRDS(list(df1, df2, df3, df4), \"./Data/CuratedOvarian_processed.rds\")\n\nNow we can see the dimensions of each study:\n\nsapply(list_gene, dim)\n\n     [,1] [,2] [,3] [,4]\n[1,]  157  195  285  117\n[2,] 1060 1060 1060 1060\n\n\nThe ultimate data for analysis has \\(N_s=(157, 195, 285, 117)\\) for \\(s=1,2, 3, 4\\), and \\(P=1060\\).\nWe scale the data so that we focus on the correlations between the genes. When the data are scaled, the variance of the genes are 1 and the off-diagonal values scales up, so does the estimated covariances matrices, which facilitates a more interpretable network analysis.\n\nY_list &lt;- readRDS(\"./Data/CuratedOvarian_processed.rds\")\nY_list_scaled &lt;- lapply(\n Y_list, function(x) scale(x, center = TRUE, scale = TRUE)\n)\nY_mat_scaled &lt;- Y_list_scaled %&gt;% do.call(rbind, .) %&gt;% as.matrix()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Case study: gene expression data</span>"
    ]
  },
  {
    "objectID": "Gene.html#fitting-the-models",
    "href": "Gene.html#fitting-the-models",
    "title": "3  Case study: gene expression data",
    "section": "3.3 Fitting the models",
    "text": "3.3 Fitting the models\nFor models fitted with MSFA::sp_fa, the function provides scaling and centering arguments. Therefore, we do not need to use the scaled data before fitting (here I use Mavis’s version of MSFA).\nStack FA and Ind FA:\n\nY_mat =  Y_list %&gt;% do.call(rbind, .) %&gt;% as.matrix()\nfit_stackFA &lt;- MSFA::sp_fa(Y_mat, k = 6, scaling = FALSE, centering = TRUE, \n                                control = list(nrun = 10000, burn = 8000))\n\nfit_indFA &lt;-\n      lapply(1:6, function(s){\n        j_s = c(8, 8, 8, 8, 8, 8)\n        MSFA::sp_fa(Y_list[[s]], k = j_s[s], scaling = FALSE, centering = TRUE,\n                    control = list(nrun = 10000, burn = 8000))\n      })\n\nMOM-SS:\n\n# Construct the membership matrix\nN_s &lt;- sapply(Y_list, nrow)\nM_list &lt;- list()\n   for(s in 1:4){\n     M_list[[s]] &lt;- matrix(1, nrow = N_s[s], ncol = 1)\n   }\nM &lt;- as.matrix(bdiag(M_list))\n\nfit_MOMSS &lt;- BFR.BE::BFR.BE.EM.CV(x = Y_mat, v = NULL, b = M, q = 20, scaling = TRUE)\n\nPFA (we don’t recommend running it. It takes 4 days and more.)\n\nN_s &lt;- sapply(Y_list, nrow)\nfit_PFA &lt;- PFA(Y=t(Y_mat_scaled), \n                        latentdim = 20,\n                       grpind = rep(1:4, \n                                     times = N_s),\n              Thin = 5,\n              Cutoff = 0.001,\n              Total_itr = 10000, burn = 8000)\n\nSUFA (takes about 10 hours):\n\nfit_SUFA &lt;- SUFA::fit_SUFA(Y_list_scaled, qmax=20,nrun = 10000)\n\nBMSFA:\n\nfit_BMSFA &lt;- MSFA::sp_msfa(Y_list, k = 20, j_s = c(4, 4, 4, 4),\n                              outputlevel = 1, scaling = TRUE, centering = TRUE, \n                               control = list(nrun = 10000, burn = 8000))\n\nTetris (we don’t recommend running it. It could take more than 10 days.):\n\nset_alpha &lt;- ceiling(1.25*4)\nfit_Tetris &lt;- tetris(Y_list_scaled, alpha=set_alpha, beta=1, nprint = 200, nrun=10000, burn=8000)\nprint(\"Start to choose big_T. It might take a long time.\")\nbig_T &lt;- choose.A(fit_Tetris, alpha_IBP=set_alpha, S=4)\nrun_fixed &lt;- tetris(Y_list_scaled, alpha=set_alpha, beta=1, \n                     fixed=TRUE, A_fixed=big_T, nprint = 200, nrun=10000, burn=8000)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Case study: gene expression data</span>"
    ]
  },
  {
    "objectID": "Gene.html#post-processing",
    "href": "Gene.html#post-processing",
    "title": "3  Case study: gene expression data",
    "section": "3.4 Post-processing",
    "text": "3.4 Post-processing\nWe use the post_xxx() functions defined in the chapter of nutrition applications to post-process for point estimates of the factor loadings, common factors, and covariance matrices.\nFor Stack FA, Ind FA and BMSFA, we need to determine the number of common factors \\(K\\) and the number of latent dimensions \\(J_s\\) for each study after the post-processing, and we need to re-run those models.\nHere we start with Stack FA, Ind FA and BMSFA:\n\n# Stack FA\nres_stackFA = post_stackFA(fit_stackFA, S=4)\n# Ind FA\nres_IndFA = post_IndFA(fit_indFA)\n# BMSFA\nres_BMSFA = post_BMSFA(fit_BMSFA)\n\n\n# Eigenvalue decomposition\nfun_eigen &lt;- function(Sig_mean) {\n  val_eigen &lt;- eigen(Sig_mean)$values\n  prop_var &lt;- val_eigen/sum(val_eigen)\n  choose_K &lt;- length(which(prop_var &gt; 0.05))\n  return(choose_K)\n}\n\nSigmaPhi_StackFA &lt;- res_stackFA$SigmaPhi\nK_StackFA &lt;- fun_eigen(SigmaPhi_StackFA)\n\nSigmaLambda_IndFA &lt;- res_IndFA$SigmaLambda\nJs_IndFA &lt;- lapply(SigmaLambda_IndFA, fun_eigen)\n\nSigmaPhi_BMSFA &lt;- res_BMSFA$SigmaPhi\nK_BMSFA &lt;- fun_eigen(SigmaPhi_BMSFA)\nSigmaLambda_BMSFA &lt;- res_BMSFA$SigmaLambda\nJs_BMSFA &lt;- lapply(SigmaLambda_BMSFA, fun_eigen)\n\n# We print the results:\nprint(paste0(\"Stack FA: K = \", K_StackFA))\n\n[1] \"Stack FA: K = 2\"\n\nprint(paste0(\"Ind FA: J_s = \", Js_IndFA))\n\n[1] \"Ind FA: J_s = 4\" \"Ind FA: J_s = 7\" \"Ind FA: J_s = 6\" \"Ind FA: J_s = 6\"\n\nprint(paste0(\"BMSFA: K = \", K_BMSFA))\n\n[1] \"BMSFA: K = 6\"\n\nprint(paste0(\"BMSFA: J_s = \", Js_BMSFA))\n\n[1] \"BMSFA: J_s = 4\" \"BMSFA: J_s = 4\" \"BMSFA: J_s = 4\" \"BMSFA: J_s = 4\"\n\n\nWe then fit the models again with the determined number of factors and latent dimensions.\n\n# Stack FA\nfit_stackFA2 &lt;- MSFA::sp_fa(Y_mat, k = K_StackFA, scaling = FALSE, centering = TRUE, \n                                control = list(nrun = 10000, burn = 8000))\n# Ind FA\nfit_indFA2 &lt;- lapply(1:4, function(s){\n  j_s = Js_IndFA[[s]]\n  MSFA::sp_fa(Y_list[[s]], k = j_s, scaling = FALSE, centering = TRUE,\n              control = list(nrun = 10000, burn = 8000))\n})\n# BMSFA\nfit_BMSFA2 &lt;- MSFA::sp_msfa(Y_list, k = K_BMSFA, j_s = Js_BMSFA,\n                            outputlevel = 1, scaling = TRUE, centering = TRUE, \n                            control = list(nrun = 10000, burn = 8000))\n\n# Again we post-process the MCMC chains.\nres_stackFA2 &lt;- post_stackFA(fit_stackFA2, S=4)\nres_IndFA2 &lt;- post_IndFA(fit_indFA2)\nres_BMSFA2 &lt;- post_BMSFA(fit_BMSFA2)\n\nFor the rest of the methods, we apply the same post-processing functions as in the nutrition applications chapter.\n\n# PFA\nres_PFA &lt;- post_PFA(fit_PFA)\n# MOM-SS\nres_MOMSS &lt;- post_MOMSS(fit_MOMSS)\n# SUFA\nres_SUFA &lt;- post_SUFA(fit_SUFA)\n# Tetris\n#res_Tetris &lt;- post_Tetris(fit_Tetris)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Case study: gene expression data</span>"
    ]
  },
  {
    "objectID": "Gene.html#visualization",
    "href": "Gene.html#visualization",
    "title": "3  Case study: gene expression data",
    "section": "3.5 Visualization",
    "text": "3.5 Visualization\nWe can visualize the common gene co-expression network using the estimated common covariance matrices. We use Gephi to visualize the networks. The SigmaPhi matrix is the common covariance matrix, and SigmaLambda is the study-specific covariance matrix.\nBefore that, we need to filter the genes so that only genes with high correlations are kept (leaving around 200 genes in the plot). We set the threshold for each model as follows:\n\nStack FA: 0.85\nMOM-SS: 0.95\nBMSFA: 0.5\nSUFA: 0.28\nPFA: 0.55\n\n\n# Filtering genes for visualization in Gephi\ngenenames &lt;- colnames(list_gene[[1]])\n\n# StackFA\nSigmaPhi_StackFA_curated &lt;- res_stackFA2$SigmaPhi\ncolnames(SigmaPhi_StackFA_curated) &lt;- rownames(SigmaPhi_StackFA_curated) &lt;- genenames\ndiag(SigmaPhi_StackFA_curated) &lt;- NA\nabove_thresh &lt;- SigmaPhi_StackFA_curated &gt; 0.85\nkeep_genes &lt;- apply(above_thresh, 1, function(x) any(x, na.rm = TRUE))\nsum(keep_genes)\n\n[1] 214\n\nSigmaPhi_StackFA_curated[abs(SigmaPhi_StackFA_curated) &lt; 0.85] &lt;- 0\nSigmaPhi_StackFA_curated_filtered &lt;- SigmaPhi_StackFA_curated[keep_genes, keep_genes]\n#write.csv(SigmaPhi_StackFA_curated_filtered, \"SigmaPhi_StackFA_curated_filtered.csv\")\n\n# MOM-SS\nSigmaPhiMOMSS &lt;- res_MOMSS$SigmaPhi\ncolnames(SigmaPhiMOMSS) &lt;- rownames(SigmaPhiMOMSS) &lt;- genenames\ndiag(SigmaPhiMOMSS) &lt;- NA\nabove_thresh &lt;- abs(SigmaPhiMOMSS) &gt; 0.95\nkeep_genes &lt;- apply(above_thresh, 1, function(x) any(x, na.rm = TRUE))\nsum(keep_genes)\n\n[1] 223\n\nSigmaPhiMOMSS[abs(SigmaPhiMOMSS) &lt; 0.95] &lt;- 0\nSigmaPhiMOMSS_filtered &lt;- SigmaPhiMOMSS[keep_genes, keep_genes]\n#write.csv(SigmaPhiMOMSS_filtered, \"SigmaPhiMOMSS_filtered.csv\")\n\n# BMSFA\nSigmaPhiBMSFA &lt;- res_BMSFA2$SigmaPhi\ncolnames(SigmaPhiBMSFA) &lt;- rownames(SigmaPhiBMSFA) &lt;- genenames\ndiag(SigmaPhiBMSFA) &lt;- NA\nabove_thresh &lt;- abs(SigmaPhiBMSFA)  &gt; 0.5\nkeep_genes &lt;- apply(above_thresh, 1, function(x) any(x, na.rm = TRUE))\nsum(keep_genes)\n\n[1] 192\n\nSigmaPhiBMSFA[abs(SigmaPhiBMSFA) &lt; 0.5] &lt;- 0\nSigmaPhiBMSFA_filtered &lt;- SigmaPhiBMSFA[keep_genes, keep_genes]\n#write.csv(SigmaPhiBMSFA_filtered, \"SigmaPhiBMSFA_filtered.csv\")\n\n# SUFA\nSigmaPhiSUFA &lt;- res_SUFA$SigmaPhi\ncolnames(SigmaPhiSUFA) &lt;- rownames(SigmaPhiSUFA) &lt;- genenames\ndiag(SigmaPhiSUFA) &lt;- NA\nabove_thresh &lt;- abs(SigmaPhiSUFA) &gt; 0.28\nkeep_genes &lt;- apply(above_thresh, 1, function(x) any(x, na.rm = TRUE))\nsum(keep_genes)\n\n[1] 191\n\nSigmaPhiSUFA[abs(SigmaPhiSUFA) &lt; 0.28] &lt;- 0\nSigmaPhiSUFA_filtered &lt;- SigmaPhiSUFA[keep_genes, keep_genes]\n#write.csv(SigmaPhiSUFA_filtered, \"SigmaPhiSUFA_filtered.csv\")\n\n# PFA\nSigmaPhiPFA &lt;- res_PFA$SigmaPhi\ncolnames(SigmaPhiPFA) &lt;- rownames(SigmaPhiPFA) &lt;- genenames\ndiag(SigmaPhiPFA) &lt;- NA\nabove_thresh &lt;- abs(SigmaPhiPFA) &gt; 0.55\nkeep_genes &lt;- apply(above_thresh, 1, function(x) any(x, na.rm = TRUE))\nsum(keep_genes)\n\n[1] 203\n\nSigmaPhiPFA[abs(SigmaPhiPFA) &lt; 0.55] &lt;- 0\nSigmaPhiPFA_filtered &lt;- SigmaPhiPFA[keep_genes, keep_genes]\n#write.csv(SigmaPhiPFA_filtered, \"SigmaPhiPFA_filtered.csv\")\n\nAttached are the final figures make with Gephi. The nodes are the genes and the edges are the correlations between the genes. The thickness of the edges indicates the strength of the correlation. The color of the nodes indicates the study they belong to. The size of the nodes indicates the number of connections they have.\nStack FA:\n\n\n\n\n\nPFA:\n\n\n\n\n\nMOM-SS:\n\n\n\n\n\nSUFA:\n\n\n\n\n\nBMSFA:\n\n\n\n\n\n\n\n\n\nFalcon, Seth, Martin Morgan, and Robert Gentleman. 2007. “An Introduction to Bioconductor’s Expressionset Class.”\n\n\nGanzfried, Benjamin Frederick, Markus Riester, Benjamin Haibe-Kains, Thomas Risch, Svitlana Tyekucheva, Ina Jazic, Xin Victoria Wang, et al. 2013. “curatedOvarianData: Clinically Annotated Data for the Ovarian Cancer Transcriptome.” Database 2013.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Case study: gene expression data</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Avalos-Pacheco, Alejandra, David Rossell, and Richard S. Savage. 2022.\n“Heterogeneous Large Datasets Integration Using\nBayesian Factor Regression.” Bayesian\nAnalysis 17 (1): 33–66.\n\n\nChandra, Noirrit Kiran, David B Dunson, and Jason Xu. 2024.\n“Inferring Covariance Structure from Multiple Data Sources via\nSubspace Factor Analysis.” Journal of the American\nStatistical Association, no. just-accepted: 1–25.\n\n\nDe Vito, Roberta, Ruggero Bellio, Lorenzo Trippa, and Giovanni\nParmigiani. 2021. “Bayesian Multistudy Factor Analysis for\nHigh-Throughput Biological Data.” The Annals of Applied\nStatistics 15 (4): 1723–41.\n\n\nDe Vito, Roberta, Briana Stephenson, Daniela Sotres-Alvarez, Anna-Maria\nSiega-Riz, Josiemer Mattei, Maria Parpinel, Brandilyn A Peters, et al.\n2022. “Shared and Ethnic Background Site-Specific Dietary Patterns\nin the Hispanic Community Health Study/Study of Latinos\n(HCHS/SOL).” medRxiv, 2022–06.\n\n\nFalcon, Seth, Martin Morgan, and Robert Gentleman. 2007. “An\nIntroduction to Bioconductor’s Expressionset Class.”\n\n\nGanzfried, Benjamin Frederick, Markus Riester, Benjamin Haibe-Kains,\nThomas Risch, Svitlana Tyekucheva, Ina Jazic, Xin Victoria Wang, et al.\n2013. “curatedOvarianData: Clinically Annotated Data for the\nOvarian Cancer Transcriptome.” Database 2013.\n\n\nGrabski, Isabella N, Roberta De Vito, Lorenzo Trippa, and Giovanni\nParmigiani. 2023. “Bayesian Combinatorial MultiStudy Factor\nAnalysis.” The Annals of Applied Statistics 17 (3):\n2212.\n\n\nRoy, Arkaprava, Isaac Lavine, Amy H. Herring, and David B. Dunson. 2021.\n“Perturbed Factor Analysis: Accounting for Group Differences in\nExposure Profiles.” The Annals of Applied Statistics 15\n(3): 1386–1404.",
    "crumbs": [
      "References"
    ]
  }
]